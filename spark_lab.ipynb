{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyspark\n",
    "# pip install findspark \n",
    "# pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac81968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 16:06:09 WARN Utils: Your hostname, Deepaks-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.17.209.32 instead (on interface en0)\n",
      "24/05/04 16:06:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/04 16:06:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/04 16:06:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/04 16:06:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"lab\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e327ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Spark (in Python or R), there is no such thing as a Dataset. Everything is a DataFrame \n",
    "spark.range(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ef1968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "        .format(\"json\") \\\n",
    "        .load(\"/Applications/MAMP/htdocs/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5665ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825dc735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d11ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfRowObjects = df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c4204",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(listOfRowObjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba280a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(listOfRowObjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1775340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(listOfRowObjects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d69fba99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|  a|     b|\n",
      "+---+------+\n",
      "|  1|{2, 2}|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: struct (nullable = true)\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: struct (nullable = true)\n",
      " |    |-- _1: long (nullable = true)\n",
      " |    |-- _2: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, (2,2))], [\"a\", \"b\"])\n",
    "df.show()\n",
    "df.printSchema(1)\n",
    "df.printSchema(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1d52812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: struct<_1:bigint,_2:bigint>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6525b978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('a', LongType(), True), StructField('b', StructType([StructField('_1', LongType(), True), StructField('_2', LongType(), True)]), True)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c2b3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,column,expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec4ff2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'a'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1acee1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'a'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8e30aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.a).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c51a4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['a']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b2cc0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('a')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "238518c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'a'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An expression created via the `expr` function is just a DataFrame column reference\n",
    "# In the simplest case, expr(\"someCol\") is equivalent to col(\"someCol\")\n",
    "expr('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ecf40a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'(a - 5)'>\n",
      "Column<'(a - 5)'>\n",
      "Column<'(a - 5)'>\n"
     ]
    }
   ],
   "source": [
    "# All the below are same! \n",
    "print(expr('a - 5'))\n",
    "print(col('a') - 5)\n",
    "print(expr('a') -5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c0c7b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eb1846",
   "metadata": {},
   "source": [
    "### Records and Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f199368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a013662",
   "metadata": {},
   "source": [
    "### Creating Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fd0c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row \n",
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9bb9e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Row('Hello', None, 1, False)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77c58e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb3c3588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a08a9",
   "metadata": {},
   "source": [
    "### Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1ede7fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|first_name|last_name|age|\n",
      "+----------+---------+---+\n",
      "|     Alice|Henderson| 25|\n",
      "|       Bob|  Sanders| 28|\n",
      "+----------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType,StructField,StringType,LongType\n",
    "myManualSchema = StructType([\n",
    "    StructField('first_name', StringType(), True),\n",
    "    StructField('last_name', StringType(), True),\n",
    "    StructField('age', LongType(), True)\n",
    "])\n",
    "\n",
    "alice = Row(\"Alice\", \"Henderson\", 25)\n",
    "bob = Row(\"Bob\", \"Sanders\", 28)\n",
    "spark.createDataFrame([alice, bob], myManualSchema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb01907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
