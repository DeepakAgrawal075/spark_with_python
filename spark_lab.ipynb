{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyspark\n",
    "# pip install findspark \n",
    "# pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac81968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"lab\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e496b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The PySpark {spark.version} version is running...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10542144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,column,expr,lit, instr, pow, round, bround\n",
    "from pyspark.sql import Row \n",
    "from pyspark.sql.types import StructType,StructField,StringType,LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e327ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Spark (in Python or R), there is no such thing as a Dataset. Everything is a DataFrame \n",
    "spark.range(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "        .format(\"json\") \\\n",
    "        .load(\"/Applications/MAMP/htdocs/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5665ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825dc735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d11ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfRowObjects = df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c4204",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(listOfRowObjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba280a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(listOfRowObjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1775340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(listOfRowObjects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69fba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1, (2,2))], [\"a\", \"b\"])\n",
    "df.show()\n",
    "df.printSchema(1)\n",
    "df.printSchema(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,column,expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb95246",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.a).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c14eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df['a']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddadb6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(col('a')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56725c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An expression created via the `expr` function is just a DataFrame column reference\n",
    "# In the simplest case, expr(\"someCol\") is equivalent to col(\"someCol\")\n",
    "expr('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the below are same! \n",
    "print(expr('a - 5'))\n",
    "print(col('a') - 5)\n",
    "print(expr('a') -5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace29dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9430c88",
   "metadata": {},
   "source": [
    "### Records and Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b436888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f905dec3",
   "metadata": {},
   "source": [
    "### Creating Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75889a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row \n",
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0527e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from dfTable\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea49c9",
   "metadata": {},
   "source": [
    "### Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b4ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType,StructField,StringType,LongType\n",
    "myManualSchema = StructType([\n",
    "    StructField('first_name', StringType(), True),\n",
    "    StructField('last_name', StringType(), True),\n",
    "    StructField('age', LongType(), True)\n",
    "])\n",
    "\n",
    "alice = Row(\"Alice\", \"Henderson\", 25)\n",
    "bob = Row(\"Bob\", \"Sanders\", 28)\n",
    "spark.createDataFrame([alice, bob], myManualSchema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f4d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sparksql-magic\n",
    "# Load the extension\n",
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from dfTable limit 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c8a951",
   "metadata": {},
   "source": [
    "### select and selectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cfc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME').show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ba117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "df.select(\n",
    "    expr('DEST_COUNTRY_NAME'),\n",
    "    col('DEST_COUNTRY_NAME'),\n",
    "    column('DEST_COUNTRY_NAME')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091da261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(col('DEST_COUNTRY_NAME'), 'DEST_COUNTRY_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(expr('DEST_COUNTRY_NAME as destination')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(expr('DEST_COUNTRY_NAME as destination').alias('alias_destination')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr('DEST_COUNTRY_NAME as destination', 'DEST_COUNTRY_NAME').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b2060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr('*', '(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withInCountry').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation over the entire DataFrame\n",
    "df.selectExpr('avg(count)','count(distinct(DEST_COUNTRY_NAME))').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a38cc5",
   "metadata": {},
   "source": [
    "### Converting to Spark Types (Literals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbbcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select('*',lit(1).alias('one')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(expr('*'),lit(1).alias('one')).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af963244",
   "metadata": {},
   "source": [
    "### Adding Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641cb3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('one', lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb02c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to add a column!\n",
    "df.withColumn('withInCountry', expr('DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b6e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to rename a column!\n",
    "df.withColumn('destination',expr('DEST_COUNTRY_NAME')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aff422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumnRenamed('DEST_COUNTRY_NAME','destination').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithLongColName = df.withColumn('Col Name With Space',expr('DEST_COUNTRY_NAME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithLongColName.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ff6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithLongColName.selectExpr('`Col Name With Space`', '`Col Name With Space` as `new col`').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithLongColName.select(col('Col Name With Space')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6238f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to escape expressions that use reserved characters/keywords\n",
    "dfWithLongColName.select(expr('`Col Name With Space`')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f0dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1940c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('count2', col('count').cast('int')).printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d21ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.filter(col('count') < 2).show(2)\n",
    "# df.filter(expr('count') < 2).show(2)\n",
    "df.filter(expr('count < 2')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where('count < 2').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e45ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col('count') < 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c45496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col('count') < 2)\\\n",
    "    .where(\"ORIGIN_COUNTRY_NAME != 'Croatia'\")\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e601e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col('count') < 2).where(col('ORIGIN_COUNTRY_NAME') != 'Croatia').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('DEST_COUNTRY_NAME','ORIGIN_COUNTRY_NAME').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56734881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('ORIGIN_COUNTRY_NAME').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6774e60f",
   "metadata": {},
   "source": [
    "### Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling without replacement, in which a subset of the observations is selected randomly, and once an observation is selected it cannot be selected again. \n",
    "# sampling with replacement, in which a subset of observations are selected randomly, and an observation may be selected more than once.\n",
    "df.sample(False,.9).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f251f1",
   "metadata": {},
   "source": [
    "### Concatenating and Appending Rows (Union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212101e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "lastRow = Row('New Country', 'New Country', 1)\n",
    "newDF = spark.createDataFrame([lastRow], df.schema)\n",
    "df.union(newDF).where('DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME').show(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985fa30e",
   "metadata": {},
   "source": [
    "### Sorting Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f48139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort('count').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd437e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy('count','DEST_COUNTRY_NAME').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204eb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(col('count'), col('DEST_COUNTRY_NAME')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(expr('count desc')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c33bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(col('count').desc(), col('DEST_COUNTRY_NAME').asc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For optimization purposes, it's sometimes advisable to sort within each partition before\n",
    "# another set of transformations\n",
    "spark.read \\\n",
    "        .format(\"json\") \\\n",
    "        .load(\"/Applications/MAMP/htdocs/Spark-The-Definitive-Guide/data/flight-data/json/*-summary.json\")\\\n",
    "        .sortWithinPartitions('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa600b",
   "metadata": {},
   "source": [
    "### Repartition & Coalesce\n",
    "#### https://mrpowers.medium.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90535a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935fdd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "rangeDF=spark.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b53f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rangeDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba3188",
   "metadata": {},
   "outputs": [],
   "source": [
    "rangeDF.write.csv('/Users/deepakagrawal/Desktop/data', 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9c71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "increasedPartitionedDF=rangeDF.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4127d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "increasedPartitionedDF.write.csv('/Users/deepakagrawal/Desktop/data', 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec968112",
   "metadata": {},
   "source": [
    "## Chapter 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.format('csv') \\\n",
    "          .option('header', 'true') \\\n",
    "          .option('inferSchema','true') \\\n",
    "          .load(\"/Applications/MAMP/htdocs/Spark-The-Definitive-Guide/data/retail-data/by-day/2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46169f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d74419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4394a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148230d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('dfTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417bf35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(lit(5), lit(\"five\"), lit(5.0)).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(lit(5), lit(\"five\"), lit(5.0)).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852fa48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(lit(5), lit(\"five\"), lit(5.0)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4005637",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit('first_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "col('first_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2baf5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "column('first_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bcdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr('first_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_to_postgres_type(spark_type):\n",
    "    if spark_type == 'string':\n",
    "        return 'TEXT'\n",
    "    elif spark_type == 'integer':\n",
    "        return 'INTEGER'\n",
    "    elif spark_type == 'long':\n",
    "        return 'BIGINT'\n",
    "    elif spark_type == 'float':\n",
    "        return 'REAL'\n",
    "    elif spark_type == 'double':\n",
    "        return 'DOUBLE PRECISION'\n",
    "    elif spark_type == 'boolean':\n",
    "        return 'BOOLEAN'\n",
    "    elif spark_type == 'date':\n",
    "        return 'DATE'\n",
    "    elif spark_type == 'timestamp':\n",
    "        return 'TIMESTAMP'\n",
    "    else:\n",
    "        return 'TEXT'\n",
    "\n",
    "table_name = 'my_table'\n",
    "create_table_sql = f\"CREATE TABLE {table_name} (\"\n",
    "for field in df.schema.fields:\n",
    "    field_name = field.name\n",
    "    field_type = field.dataType.typeName()\n",
    "    postgres_field_type = spark_to_postgres_type(field_type)\n",
    "    create_table_sql += f\"{field_name} {postgres_field_type}, \"\n",
    "\n",
    "create_table_sql = create_table_sql[:-2] # Remove the last comma and the space\n",
    "create_table_sql += ')'\n",
    "print(create_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b958ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col('InvoiceNo') != 536365) \\\n",
    "  .select('InvoiceNo','Description') \\\n",
    "  .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where('InvoiceNo = 536365').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f1f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where('InvoiceNo <> 536365').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5364587",
   "metadata": {},
   "outputs": [],
   "source": [
    "priceFilter = col('UnitPrice') > 600\n",
    "descripFilter = instr(df.Description, 'POSTAGE') >= 1\n",
    "df.where(df.StockCode.isin('DOT')).where(priceFilter | descripFilter).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOTCodeFilter = col('StockCode') == 'DOT'\n",
    "priceFilter = col('UnitPrice') > 600\n",
    "descripFilter = instr(df.Description, 'POSTAGE') >= 1\n",
    "df.withColumn('isExpensive', DOTCodeFilter & (priceFilter | descripFilter)) \\\n",
    "  .where('isExpensive') \\\n",
    "  .select('UnitPrice', 'StockCode', 'Description',  'isExpensive').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a7081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Express Filters as SQL statements \n",
    "df.withColumn('isExpensive', expr('NOT UnitPrice <= 250')) \\\n",
    "  .where('isExpensive') \\\n",
    "  .select('Description','UnitPrice').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a0e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with null data when creating Boolean expressions\n",
    "df.where(col('Description').eqNullSafe(\"hello\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec3858",
   "metadata": {},
   "outputs": [],
   "source": [
    "fabricatedQuantity = pow(col('Quantity') * col('UnitPrice'), 2) + 5\n",
    "df.select(expr('CustomerID'), fabricatedQuantity.alias('realQuantity')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e3ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As SQL expression\n",
    "df.selectExpr('CustomerID', '(POWER((Quantity * UnitPrice), 2) + 5) as realQuantity').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58da7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounding\n",
    "df.select(round(lit('2.5')), bround(lit('2.5'))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60bffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Summary Statistics \n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a unique ID to each row\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df.select(monotonically_increasing_id()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf55853",
   "metadata": {},
   "source": [
    "### Working With Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98386d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "df.select(col('Description')).show(10, False)\n",
    "df.select(initcap(col('Description'))).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cadc309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, upper\n",
    "df.select(col('Description'), \\\n",
    "          lower(col('Description')), \\\n",
    "          upper(lower(col('Description')))) \\\n",
    "  .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac04ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, trim, lpad, rpad\n",
    "df.select(\n",
    "    ltrim(lit('     HELLO      ')).alias('ltrim'),\n",
    "    rtrim(lit('     HELLO      ')).alias('rtrim'),\n",
    "    trim(lit('     HELLO      ')).alias('trim'),\n",
    "    lpad(lit('HELLO'), 3, '$').alias('lpad'),\n",
    "    rpad(lit('HELLO'), 10, '#').alias('rpad')\n",
    "  ).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7af68",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "regex_string = 'BLACK|WHITE|RED|GREEN|BLUE'\n",
    "df.select(\n",
    "    col('Description'),\n",
    "    regexp_replace(\n",
    "        col('Description'), regex_string, '{COLOR}').alias('color_clean')\n",
    "    ).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "df.select(\n",
    "    col('Description'),\n",
    "    translate(\n",
    "        col('Description'), 'LET', '137')\n",
    "    ).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a3ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling out the first mentioned color:\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "extract_string = 'BLACK|WHITE|RED|GREEN|BLUE'\n",
    "df.select(\n",
    "    col('Description'),\n",
    "    regexp_extract(\n",
    "        col('Description'), extract_string, 0).alias('color_clean')\n",
    "    ).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7195b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr \n",
    "containsBlack = instr(col('Description'), 'BLACK') >=1\n",
    "containsWhite = instr(col('Description'), 'WHITE') >=1\n",
    "df.withColumn('hasSimpleColor', containsBlack | containsWhite) \\\n",
    "  .where('hasSimpleColor') \\\n",
    "  .select('Description') \\\n",
    "  .show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34473bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, locate\n",
    "simpleColors = ['black', 'white', 'red', 'green', 'blue']\n",
    "def color_locator(column, color_string):\n",
    "    return locate(color_string.upper(), column).cast('boolean').alias('is_' + color_string)\n",
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
    "selectedColumns.append(expr('*'))\n",
    "df.select(*selectedColumns).where(expr('is_white AND is_red')).select('Description').show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25582ce2",
   "metadata": {},
   "source": [
    "### Working with Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acee1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "dateDF = spark.range(1).withColumn('today', current_date()).withColumn('now', current_timestamp())\n",
    "dateDF.show(1, False)\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b872145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "dateDF.select(date_sub(col('today'), 5), date_add(col('today'), 5)).show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name datediff is a shorthand derived from the words \"date\" and \"difference\"\n",
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "dateDF.withColumn('week_ago', date_sub(col('today'), 7)) \\\n",
    "      .select(datediff(col('week_ago'), col('today'))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF.select(\n",
    "    to_date(lit('2016-01-01')).alias('start'), \n",
    "    to_date(lit('2017-01-01')).alias('end')) \\\n",
    "    .select(months_between(col('start'), col('end'))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1).withColumn('date', lit('2017-01-01')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792dd111",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1).withColumn('date', lit('2017-01-01')) \\\n",
    "     .select(to_date(col('date'))).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark will not throw an error if it can't parse the date; rather, it will just return null\n",
    "dateDF.select(to_date(lit('2016-20-12')), to_date(lit('2016-12-20'))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ed376",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateFormat = 'yyyy-dd-MM'\n",
    "dateDF.select(to_date(lit('2016-20-12'), dateFormat), to_date(lit('2016-12-20'), dateFormat)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7bec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateFormat = 'yyyy-MM-dd'\n",
    "dateDF.select(to_date(lit('2016-20-12'), dateFormat), to_date(lit('2016-12-20'), dateFormat)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8736b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "dateFormat = 'yyyy-dd-MM'\n",
    "dateDF.select(to_date(lit('2016-20-12'), dateFormat).alias('date')) \\\n",
    "      .select(to_timestamp(col('date'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24c0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
